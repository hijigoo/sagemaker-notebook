{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# With LangChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class CustomCallBackHandler(BaseCallbackHandler):\n",
    "\n",
    "    # 아래 함수에서 Frontend 와 Socket으로 연결해서 실시간으로 UI로 전달.\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"max_tokens_to_sample\": 200,  # min:0, max:4096, default:200\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "    \"temperature\": 1,  # min:0, max:1, default:0.5\n",
    "    \"top_p\": 1,  # min:0, max:1, default:1\n",
    "    \"top_k\": 225,  # min:0, max:500, default:250\n",
    "}\n",
    "\n",
    "model_id = \"anthropic.claude-v2:1\"\n",
    "llm = Bedrock(\n",
    "    model_id=model_id,\n",
    "    streaming=True,\n",
    "    model_kwargs=parameters,\n",
    "    callbacks=[CustomCallBackHandler()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "conversation.predict(input=\"Hi there!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Without LangChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_region = \"us-west-2\"\n",
    "\n",
    "def get_bedrock_client():\n",
    "    return boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=bedrock_region,\n",
    "    )\n",
    "\n",
    "def get_stream_from_bedrock_client(model_id: str, parameters: dict):\n",
    "    bedrock_client = get_bedrock_client()\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "    return bedrock_client.invoke_model_with_response_stream(\n",
    "        body=json.dumps(parameters),\n",
    "        modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "\n",
    "def generate_stream_answer_with_context(question: str,\n",
    "                                        max_tokens: int = 200, temperature: float = 0,\n",
    "                                        top_p: float = 0.9,\n",
    "                                        top_k: int = 250, model_id: str = 'anthropic.claude-v2:1'):\n",
    "    prompt = \"\".join([f\"\\n\\nHuman: {question}\", \"\\n\\nAssistant:\"])\n",
    "\n",
    "    parameters = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens_to_sample\": int(max_tokens),  # min:0, max:8,000, default:200\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "        \"temperature\": float(temperature),  # min:0, max:1, default:0.5\n",
    "        \"top_p\": float(top_p),  # min:0, max:1, default:1\n",
    "        \"top_k\": int(top_k)  # min:0, max:500, default:250\n",
    "    }\n",
    "\n",
    "    bedrock_client = get_bedrock_client()\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "    response = bedrock_client.invoke_model_with_response_stream(\n",
    "        body=json.dumps(parameters),\n",
    "        modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "\n",
    "    return response.get('body')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"안녕 베드락에 대해서 설명해줘\"\n",
    "\n",
    "stream = generate_stream_answer_with_context(question=\"안녕?!\")\n",
    "\n",
    "output = []\n",
    "i = 1\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "            text = chunk_obj['completion']\n",
    "\n",
    "            #아래 부분을 Socket 연결해서 실시간으로 UI 로 전달.\n",
    "            print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
